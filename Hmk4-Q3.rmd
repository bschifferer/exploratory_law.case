---
title: "Homework #4 - Q3"
author: "Benedikt Schifferer - bds2141"
date: 14.11.2018
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)
```

```{r}
# Load libraries
library(tidyverse)
library(gridExtra)
library(boot)
library(cluster)
library(scales)
library(carData)
library(extracat)
library(pgmm)
library(parcoords)
library(grid)
library(tidyquant)
library(choroplethr)
library(choroplethrZip)
```

### 0. Background

This is the solution to Homework4 - Q3, analyzing a variable of our final project dataset. As discussed with Prof. Robbins, we will analyse the https://case.law/ data set, which is a collection of ~6 mio. law cases (From ~1650 to 2018) and was published, recently. Therefore, there are no (almost none) analysis available. Our goal is to analyse trends related to case subject (e.g. criminal vs. reimbursement vs. housing) and validate hypthothesis based on additional data (e.g. matching to economic growth per year). As we requested access to the full dataset, the current analysis is based on a subet of data and will be rerun on the full dataset, when received (aligned with Prof. Robbins). In particular, this data set is important as one of the biggest public law case collection and will enables more data science project in that domain.<br><br>

### 1. Quesiton

The goal of this assignment is to analysis the variable text, which is an autommatic optical character recognition from the scanned pages. An example json of all available variables can be found here: https://github.com/bschifferer/exploratory_law.case/blob/master/example.json<br>
<br>
Many information can be extracted from the variable text, such as involved companies, category of law suit, importance, etc.. Although we will get domain knowledge from law students, we want to analyse descriptive the frquency of the words and extract meaningful categories from it.<br>
<br>
Instead of using the frequency count of a word, I focus on the document frequency of a word. If a word appears in a document, then the frequency is 1 else 0.<br><br>
There should be three cases:<br>
1. Words with high document frequency - appears in almost all document and have 0 meaning (e.g. fill words like "is")
2. Words with low document frequency - appears in almost no document and have 0 meaning (e.g. names)
3. Words with medium document frequency - appears frequent enough to be relevant but not always to be never revelant

Let's start:

```{r}
setwd('/Users/bennys/Projects/Columbia/01_EDV/03_Project/exploratory_law.case')
doc_freq <- read_csv(file = 'doc_freq_dict.csv', )
doc_freq <- as.data.frame(doc_freq)
colnames(doc_freq) <- c('Word', 'No')
str(doc_freq)
```

```{r}
ggplot(aes(x = No), data = doc_freq) + 
  geom_histogram(position="identity", colour="grey40") +
  xlab('Document frequency') +
  ggtitle('Document frequency per word')
```



